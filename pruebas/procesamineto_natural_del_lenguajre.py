# -*- coding: utf-8 -*-
"""Procesamineto Natural del Lenguajre.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q6nk5AHswSbcADhXo2asL3lIqpbVDQlI
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\t', quoting = 3)
#\t Es para identificar la separacion en vez de ',' sera tabulador 
#QUOTING 3 para ignorar las comillas dobles
dataset.head

#DEBEMOS LIMPIAR EL TEXTO...





"""LIMPIEZA DE TEXTO 
Así que empecemos por limpiar el texto de cada una de las Review.
Para ello dejadme cerrar aquí.
La matriz está quetenía y colocar una sección llamado limpieza de texto.
Parece que vamos a coger aquí el desengrasante y la Karcher y vamos a darle duro a la parte de texto
pero no limpiar el texto se trata de hacer un bucle para revisar nunca mejor dicho todas las reviews
todas las valoraciones y aplicar las técnicas de limpieza que os he estado comentando hasta el momento
en el vídeo así que.
Bueno pues vamos a ello porque tenemos muchísimo que limpiar de acuerdo.
Hay que empezar paso a paso a paso palabra a palabra y aplicar una por una.
El proceso de eliminación.
El proceso de quitar palabras inútiles el proceso de limpiar palabra a palabra en una primera ronda
luego pasaremos a la siguiente siguiente a la siguiente a la siguiente y así sucesivamente hasta haber
filtrado limpiado todas y cada una de las reviews.
Pues bueno vamos a hacerlo ahora y con qué empezaremos.
"""
##PRIMERA FASE
import re             # que son las iniciales de regular expresiones expresiones regulares son herramientas perfectas para limpiar y tratar los textos de acuerdo.
import nltk           #natural lenguaje talk kit**   el proceso de búsqueda el proceso de limpieza que sea capaz de deshacerse de este  tipo de palabras irrelevantes para predecirse a una determinada valoración tiene significado positivo o significado negativo.
nltk.download('stopwords')       # descargar el conjunto de palabras inutiles
from nltk.corpus import stopwords# el cuerpo de palabras dentro de nltk, i need import las stopsword
from nltk.stem.porter import PorterStemmer    # crear una clase utilizar una librería que se encargue de ese proceso de traducción a que cada una de estas palabras aparezca con su infinitivo respectivo
corpus = []                     #iniciamos una lista vacia, para llenarla de criricas limpias, todo el texto analizado  limpiado 
for i in range(0, 1000):        #ponemos el nro de reviews en el dataset
    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])   # cambiar todo que no sea ^a-zA-Z] por '_'
    review = review.lower()     # para convertir en minusculas 
    review = review.split()     # para convertirlo en una lista y separar las palabras
    ps = PorterStemmer()        # invocamos ps
    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]    #eliminamos las palabras irrelevantes y si no es una stopword se va a review, cabiamos el lenguaje a gusto
                                #set(stopwords.words('english') es una lista #### unimos la funcion ps para convertir en infinitivo las palabras sobrevivientees
    review = ' '.join(review)   #juntar las palabras, separando por ' ' 
    corpus.append(review)       # adicionamos la linea de palabras a la lista de palabras limpias

##SEGUNDA FASE
##APLICAMOS EL MODELO BAG OF WORDS
"""
Pero bueno básicamente es muy simple vamos a hacer es simplemente tomar todo lo que son palabras diferentes
no sé cuántas habrá habrá seiscientas o mil o diez mil palabras diferentes y cada una de esas palabras
pasará a ser una columna,el saco de palabras la bolsa de palabras tendrá las mismas filas que tenía
antes pero ahora aparecerá una columna para cada palabra esencialmente diferente..
"""



from sklearn.feature_extraction.text import CountVectorizer # Transformara los textos en una vectores de frecuencias, para ver de qué tantas veces aparece una determinada palabra
cv = CountVectorizer(max_features = 1500)                   # creamos el objeto para el vector de frecuencias, me quedare con las 1500 words palabras mas relevantes
X = cv.fit_transform(corpus).toarray()                      # Crear la matriz dispersa X, ajstar miarat¿ todas las palabras  fit para crear el modelo, para sacar el saco de palabras
y = dataset.iloc[:, 1].values                               # variables independientes. Sacar la ultima columna  para la prediccion

##SELECCIONAR UN ALGORITMO DE CLASIFICACION

#Dividimos en train y test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

# Training the Random Forest Classification model on the Training set
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)

"""from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)"""

y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

(87+57)/(87+10+46+57)

